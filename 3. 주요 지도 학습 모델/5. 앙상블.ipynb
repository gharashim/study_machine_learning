{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 앙상블의 종류\n",
    "## 앙상블이란?\n",
    "앙상블은 기본 모델(base model)이라 불리는 여러 작은 모델을 개별적으로 학습하여 종합한 모델 입니다.\n",
    "\n",
    "- 공짜 점심 이론 : 모든 문제에 대해 다른 모델보다 항상 우월한 모델은 없음 → 앙상블 도입의 배경\n",
    "- 앙상블 모델은 서로 다른 여러 모델을 바탕으로 집단 지성을 표방함\n",
    "- 캐글에서 가장 많이 사용되는 알고리즘이 XGBoost와 LightGBM 등의 앙상블 모델임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 종류 개요\n",
    "앙상블은 여러 모델을 학습하는 방법과 예측 결과를 종합하는 방법에 따라 앙상블을 구분할 수 있습니다.\n",
    "\n",
    "앙상블의 종류\n",
    "- 투표(voting)\n",
    "    - 전체 데이터를 사용하여 독립적으로 학습\n",
    "    - 예측 결과의 평균 및 최빈값으로 종합\n",
    "- 배깅(bagging)\n",
    "    - 붓스트랩한 데이터를 사용하여 독립적으로 학습\n",
    "    - 예측 결과의 평균 및 최빈값으로 종합\n",
    "- 부스팅(boosting)\n",
    "    - 전체 데이터를 사용하여 순차적으로 학습\n",
    "    - 예측 결과의 평균 및 최빈값으로 종합\n",
    "- 스태킹(stacking)\n",
    "    - 전체 데이터를 사용하여 독립적으로 학습\n",
    "    - 각 모델의 예측 결과를 특징으로 하는 모델을 사용하여 최종 예측 결과를 종합"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 투표 모델\n",
    "투표 모델은 전체 데이터를 사용해 독립적으로 학습한 여러 모델의 예측 결과를 평균 및 최빈값으로 종합하는 모델 입니다.\n",
    "- 같은 데이터로 여러 모델을 만들기 때문에 각 모델의 예측 결과가 비슷할 수 있음\n",
    "- 여러 모델의 예측 결과를 종합한 것과 하나의 모델의 예측 결과에 큰 차이가 없어 여러 모델을 학습하는 게 의미가 없을 수 있음\n",
    "- 투표 모델을 사용할 땐 서로 크게 다른 모델 여러 개를 사용해야 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 부스팅\n",
    "부스팅은 한 모델을 학습하고 계산한 오차를 보정하는 모델을 순차적으로 학습합니다.\n",
    "- 여기서 i번째 모델은 (i-1)번째 모델이 제대로 예측 못한 샘플에 가중치를 부여한 데이터로 학습함.\n",
    "- 컴퓨팅 자원만 충분하다면 동시에 여러 모델을 학습할 수 있는 배깅과 투표 모델, 스태킹 모델과 다르게, 부스팅 모델은 이전 모델이 학습돼야만 다음 모델을 학습할 수 있으므로 학습 시간이 오래 걸림\n",
    "- 이러한 단점을 보완하기 위해 대표적인 부스팅 모델인 XGBoost와 LightGBM은 결정 나무를 학습하는 과정에서 병렬 연산(parallel computing)을 사용함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스태킹\n",
    "스태킹은 개별 모델이 예측한 결과를 입력으로 하는 메타 모델(meta model)을 사용해 예측 결과를 종합합니다.\n",
    "- 메타 모델은 개별 모델의 예측값을 특징으로 사용하고 데이터의 라벨을 그대로 라벨에 사용해 학습함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 결정 나무 기반의 앙상블 모델\n",
    "## 왜 결정 나무인가?\n",
    "결정 나무는 앙상블 모델의 기본 모델로 많이 사용됩니다.\n",
    "- 결정 나무는 대표적인 앙상블 모델인 랜덤 포레스트, XGBoost, LightGBM의 기본 모델임\n",
    "- 얕은 결정 나무는 적은 데이터로 학습하더라도 과적합될 가능성이 매우 작음\n",
    "- 배깅은 붓스트랩된 작은 데이터로 모델을 학습하므로 과적합 위험이 적은 모델을 사용해야 함\n",
    "- 시드가 다르다면 같은 데이터로 학습한 결정 나무일지라도 다르게 분지될 수 있어, 다양한 조건으로 데이터 공간을 탐색할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤포레스트\n",
    "랜덤 포레스트는 기본 모델이 결정 나무인 배깅 모델 입니다.\n",
    "- 랜덤 포레스트의 주요 하이퍼 파라미터는 기본 모델의 개수와 붓스트랩 관련 하이퍼 파라미터를 제외하면 결정 나무의 하이퍼 파라미터와 같음\n",
    "- 기본 모델의 개수가 많을수록 모델이 복잡해지지만 각 모델이 학습에 사용하는 데이터 크기는 비슷하므로 과적합 위험이 커진다고 보기 힘듦\n",
    "- 그러나 작은 학습 데이터로 개별 모델을 학습하므로 나무의 깊이는 5 이하로 낮게 설정해주는 것이 적절함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost는 부스팅 방식으로 여러 개의 결정 나무를 병렬로 학습하는 앙상블 모델 입니다.\n",
    "\n",
    "![nn](images/XGBoost.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "LightGBM은 XGBoost와 비슷하지만 잎 노드 중심 분할(leaf wise) 방식을 사용해 깊고 비대칭적인 트리를 만듭니다.\n",
    "![nn](images/LightGBM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 실습\n",
    "## 예제 데이터 불러오기\n",
    "앙상블 모델을 학습할 예제 데이터를 불러옵니다.\n",
    "\n",
    "예제 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df1 = pd.read_csv('../data/classification/optdigits.csv') # 다중 분류 데이터\n",
    "df2 = pd.read_csv('../data/regression/baseball.csv') # 회귀 데이터\n",
    "\n",
    "X1 = df1.drop('y', axis=1)\n",
    "y1 = df1['y']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=2022)\n",
    "\n",
    "X2 = df2.drop('y', axis=1)\n",
    "y2 = df2['y']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트 모델 학습\n",
    "랜덤 포레스트는 sklearn.ensemble 모듈의 RandomForestClassifier와 RandomForestRegressor를 이용해 구현할 수 있습니다.\n",
    "\n",
    "주요 인자\n",
    "- n_estimators\n",
    "    - 결정 나무의 개수\n",
    "    - 기본값 : 100\n",
    "- max_depth\n",
    "    - 최대 깊이\n",
    "    - 기본값 : None\n",
    "- max_feature\n",
    "    - 개별 결정 나무를 학습하는 데 사용하는 특징 개수 혹은 비율\n",
    "    - 전체 특징을 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트와 결정 나무 비교\n",
    "랜덤 포레스트와 결정 나무를 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811387900355871 0.9423487544483986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC, DecisionTreeRegressor as DTR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, RandomForestRegressor as RFR\n",
    "from sklearn.metrics import *\n",
    "dtc = DTC(max_depth=10, random_state=2022).fit(X1_train, y1_train)\n",
    "rfc = RFC(max_depth=5, random_state=2022).fit(X1_train, y1_train)\n",
    "dtc_pred = dtc.predict(X1_test)\n",
    "rfc_pred = rfc.predict(X1_test)\n",
    "dtc_acc = accuracy_score(y1_test, dtc_pred)\n",
    "rfc_acc = accuracy_score(y1_test, rfc_pred)\n",
    "print(dtc_acc, rfc_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라인 1\n",
    "    - DecisionTreeClassifier와 DecsionTreeRegressor를 각각 DTC와 DTR로 축약하여 불러 왔습니다.\n",
    "    - 이처럼 둘 이상의 함수를 축약해서 불러올 때는 import 함수 1 as 축약어1, 함수2 as 축약어2 와 같이 쉼표(,)로 연결하여 불러옵니다.\n",
    "- 라인 4\n",
    "    - 최대 깊이가 10인 결정 나무를 학습합니다.\n",
    "- 라인 5\n",
    "    - 최대 깊이가 5인 결정 나무를 100개(기본값) 학습 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 포레스트와 결정 나무를 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=5, max_features='sqrt', random_state=40194941)\n",
      "DecisionTreeClassifier(max_depth=5, max_features='sqrt', random_state=101951500)\n"
     ]
    }
   ],
   "source": [
    "print(rfc[0])\n",
    "print(rfc[50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0번째 요소와 50번째 요소 모두 DecisionTreeClassifier임을 알 수 있음\n",
    "- random_state를 2022로 설정했는데도 개별 모델의 random_state는 다름. 그 이유는 사이킷런에서는 다양한 모델로 구성된 랜덤 포레스트를 학습하기 위해 랜덤 포레스트의 시드를 바탕으로 결정 나무의 시드를 난수로 정하기 때문\n",
    "- 즉, 랜덤 포레스트의 시드가 개별 결정 나무의 시드를 결정하는 것이지, 랜덤 포레스트의 시드와 개별 결절 나무의 시드가 같은 것은 아닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579.9513012477719 530.4179945238225\n"
     ]
    }
   ],
   "source": [
    "dtr = DTR(max_depth=10, random_state=2022).fit(X2_train, y2_train)\n",
    "rfr = RFR(max_depth=5, random_state=2022).fit(X2_train, y2_train)\n",
    "\n",
    "dtr_pred = dtr.predict(X2_test)\n",
    "rfr_pred = rfr.predict(X2_test)\n",
    "\n",
    "dtr_mae = mean_absolute_error(y2_test, dtr_pred)\n",
    "rfr_mae = mean_absolute_error(y2_test, rfr_pred)\n",
    "print(dtr_mae, rfr_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 모델 학습\n",
    "XGBoost 모듈의 XGBClassifier와 XGBRegressor를 이용하면 각각 XGBoost 방식의 분류 모델과 회귀 모델을 학습할 수 있습니다.\n",
    "\n",
    "XGBoost 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.2-py3-none-win_amd64.whl (89.1 MB)\n",
      "     ---------------------------------------- 89.1/89.1 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.23.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from xgboost) (1.9.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주요 인자\n",
    "- n_estimators\n",
    "    - 결정 나무의 개수\n",
    "- max_depth\n",
    "    - 최대 깊이\n",
    "- laerning_rate\n",
    "    - 학습률"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost 모듈의 XGBClassifier와 XGBRegressor를 이용하면 각각 XGBoost 방식의 분류 모델과 회귀 모델을 학습할 수 있습니다.\n",
    "\n",
    "XGBoost와 결정 나무 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772241992882562 561.5312666949104\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier as XGBC, XGBRegressor as XGBR\n",
    "xgbc = XGBC(random_state=2022, learning_rate=0.1, max_depth=5).fit(X1_train, y1_train)\n",
    "xgbr = XGBR(random_state=2022, learning_rate=0.1, max_depth=5).fit(X2_train, y2_train)\n",
    "xgbc_pred = xgbc.predict(X1_test)\n",
    "xgbr_pred = xgbr.predict(X2_test)\n",
    "\n",
    "acc = accuracy_score(xgbc_pred, y1_test)\n",
    "mae = mean_absolute_error(xgbr_pred, y2_test)\n",
    "print(acc, mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결정 나무의 정확도(0.8811)와 MAE(579.9513)보다 더 좋은 성능을 보임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM 모델 학습\n",
    "lightgbm 모듈의 LGBMClassifier와 LGBMRegressor를 이용하면 각각 LightGBM 방식의 분류 모델과 회귀 모델을 학습할 수 있습니다.\n",
    "\n",
    "lightgbm 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from lightgbm) (1.9.0)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from lightgbm) (1.23.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rkfka\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Installing collected packages: wheel, lightgbm\n",
      "Successfully installed lightgbm-3.3.3 wheel-0.38.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'c:\\Users\\rkfka\\AppData\\Local\\Programs\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install lightgbm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주요 인자\n",
    "- n_estimators\n",
    "    - 결정 나무의 개수\n",
    "- max_depth\n",
    "    - 최대 깊이\n",
    "- num_leaves\n",
    "    - 최대 잎 노드 개수\n",
    "- laerning_rate\n",
    "    - 학습률"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm 모듈의 LGBMRegressor를 이용하면 각각 LightGBM 방식의 분류 모델과 회귀 모델을 학습할 수 있습니다.\n",
    "\n",
    "LightGBM과 결정 나무 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814946619217082 572.6819576553671\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier as LGBC, LGBMRegressor as LGBR\n",
    "lgbc = LGBC(random_state=2022, learning_rate=0.1, num_leaves=32).fit(X1_train, y1_train)\n",
    "lgbr = LGBR(random_state=2022, learning_rate=0.1, num_leaves=32).fit(X2_train, y2_train)\n",
    "lgbc_pred = lgbc.predict(X1_test)\n",
    "lgbr_pred = lgbr.predict(X2_test)\n",
    "\n",
    "acc = accuracy_score(lgbc_pred, y1_test)\n",
    "mae = mean_absolute_error(lgbr_pred, y2_test)\n",
    "print(acc, mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류 성능은 LightGBM이 가장 좋지만, 회귀 성능은 랜덤 포레스트와 XGBoost보다는 약간 떨어지고 결정 나무보다는 좋음\n",
    "- 그러나 데이터에 따라 결과가 다를 수 있으므로 이 결과만 보고 모델과 하이퍼파라미터를 선택하는 것은 매우 위험합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "107109ef1f3d2c1d8ef2422d7f3c7553b0305ed5b701a76fbc5bdcb10e21ee2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
