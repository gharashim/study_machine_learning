{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 모델 구조와 작동 과정\n",
    "## 모델 구조\n",
    "신경망의 구조는 인간의 뇌 작동 과정을 모방합니다.\n",
    "- 신경망은 입력층, 은닉층, 출력층으로 구성됨\n",
    "- 각 층은 하나 이상의 노드로 구성됨\n",
    "- 신경망의 각 노드는 다음 층에 있는 각 노드와 연결됨\n",
    "- 입력층에 있는 각 노드는 특징을 입력받는 역할을 하므로, 입력 노드 수는 특징 개수와 같음\n",
    "- 출력층에 있는 각 노드는 최종 결과를 출력하는 역할을 하므로 출력 노드 수는 라벨 혹은 클래스의 개수가 됨\n",
    "- 입력층과 출력층은 사용자가 구조를 지정하지 않지만, 은닉층은 사용자가 직접 지정해야 함\n",
    "\n",
    "신경망의 은닉층은 복잡도와 직결되는 중요한 하이퍼 파리미터 임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 작동 과정\n",
    "신경망은 가중합과 활성화 함수라는 두 가지 연산을 반복하여 라벨을 예측합니다.\n",
    "- 입력 노드를 제외한 모든 노드의 연산 과정\n",
    "    - 입력값 가중합\n",
    "    - 활성화 함수에 입력\n",
    "    - 다음 노드에 전달\n",
    "\n",
    "신경망은 가중합 연산이 여러 번 반복되는 모델이므로 특징의 스케일에 크게 영향을 받음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 개의 입력 노드에 특징값 𝑥1과 𝑥2가 각각 저장됨\n",
    "- 𝑥1과 𝑥2가 각 은닉 노드로 전달되며, 이 과정에서 가중합과 활성화 함수 연산이 수행됨\n",
    "    - 𝑧1 =𝑎1 𝑤1𝑥1 +𝑤3𝑥2\n",
    "    - 𝑧2 =𝑎1 𝑤2𝑥1 +𝑤4𝑥2\n",
    "\n",
    "- 𝑧1과 𝑧2가 각 출력 노드로 전달되며, 이 과정에서 가중합과 활성화 함수 연산이 수행됨\n",
    "    - 𝑜1 =𝑎2 𝑤5𝑧1 +𝑤7𝑧2\n",
    "    - 𝑜2 =𝑎2 𝑤6𝑧1 +𝑤8𝑧2\n",
    "\n",
    "- 출력 노드의 값 𝑜1과 𝑜2를 출력함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 : 소프트 맥스\n",
    "소프트 맥스는 분류 결과를 종합하는 함수 입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수\n",
    "활성화 함수는 신경망에 비선형성을 부여하는 함수로 렐루 함수가 주로 사용 됩니다\n",
    "- 주요 활성화 함수\n",
    "  - 시그모이드 함수\n",
    "  - 렐루 함수\n",
    "  - 리키 렐루 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 학습과 주요 하이퍼 파라미터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류 역전파 알고리즘\n",
    "신경망은 오류 역전파 알고리즘(back propagation algorithm)을 통해 학습됩니다.\n",
    "\n",
    "1. 모든 가중치를 임의로 초기호 하고 이터레이션 수를 0으로 초기화\n",
    "2. 이터레이션 수를 1만큼 증가시키고, 학습 데이터를 모델에 입력하여 예측값과 오차를 계산\n",
    "3. 오차를 바탕으로 출력층에 가까운 가중치부터 순서대로 업데이트\n",
    "4. 종료 조건(만족 못하면 2번으로)\n",
    "5. 학습 종료"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내리막 경사법\n",
    "오류 역전파 알고리즘은 내리막 경사법의 일종으로, 내리막 경사법을 알아야 학습률과 초기 가중치의 중요성을 제대로 이해할 수 있습니다.\n",
    "1. 임의의 해 선정\n",
    "2. 접선 기울기의 반대 방향으로 이동\n",
    "3. 접선 기울기가 0인 지점에서 정지\n",
    "\n",
    "임의로 설정한 초기 해에 따라 신경망의 성능이 크게 좌우되므로 시드를 잘 설정해야 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류 역전파 알고리즘은 내리막 경사법의 일종으로, 내리막 경사법을 알아야 학습률과 초기 가중치의 중요성을 제대로 이해할 수 있습니다.\n",
    "- 학습률이 클수록 더 빠르게 수렴하지만, 발산할 위험이 있음\n",
    "- 또한, 지역 최적을 피할 수도 전역 최적을 못 찾을 수도 있음\n",
    "- 최대 이터레이션 횟수에 도달해서 알고리즘이 종료되는 경우가 많고,\n",
    "- 최대 이터레이션 횟수가 너무 크면 과적합 가능성이, 너무 작으면 과소 적합 가능성이 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구조\n",
    "신경망에는 여러 하이퍼 파라미터가 있으나 대부분 널리 쓰이는 값을 사용하는 것이 좋습니다.\n",
    "\n",
    "그런데 신경망의 모델 구조는 그렇지 않으며, 신경망의 성능은 사실상 모델 구조가 결정한다고 해도 과언이 아닙니다.\n",
    "- 은닉층의 구조가 복잡할수록 모델이 복잡해짐"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 사이킷런 실습\n",
    "\n",
    "예제 데이터 불러오기\n",
    "- 신경망을 학습할 예제 데이터를 불러 옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('../data/regression/puma32h.csv')\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습\n",
    "신경망은 사이킷런의 neural_network 모듈의 MLPClassifier와 MLPRegressor를 이용해 구현할 수 있습니다.\n",
    "\n",
    "주요 인자\n",
    "- hidden_layer_sizes\n",
    "    - 튜플 형태의 은닉층 구조\n",
    "    - 기본값 : (100,)\n",
    "- activation\n",
    "    - 활성화 함수\n",
    "    - 기본값 : 'relu'\n",
    "- max_iter\n",
    "    - 최대 이터레이션 횟수\n",
    "    - 기본값 : 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습 및 평가 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060378364874099415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor as NN\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "model = NN(random_state=2022).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = MAE(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구조 비교\n",
    "hidden_layer_sizes 인자를 바꿔가며 다양한 모델 구조를 정의하고 그 성능을 확인해보겠습니다.\n",
    "\n",
    "모델 구조 비교 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NN(hidden_layer_sizes=(5,), random_state=2022).fit(X_train, y_train)\n",
    "model2 = NN(hidden_layer_sizes=(10, 10), random_state=2022).fit(X_train, y_train)\n",
    "model3 = NN(hidden_layer_sizes=(100, 100, 100), random_state=2022).fit(X_train, y_train)\n",
    "\n",
    "y1_pred = model1.predict(X_test)\n",
    "y2_pred = model2.predict(X_test)\n",
    "y3_pred = model3.predict(X_test)\n",
    "\n",
    "mae1 = MAE(y_test, y1_pred)\n",
    "mae2 = MAE(y_test, y2_pred)\n",
    "mae3 = MAE(y_test, y3_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라인 1 - 3\n",
    "    - 은닉층의 구조가 (5,), (10,10), (100,100,100)인 시녁ㅇ망 model1, model2, model3을 학습했습니다.\n",
    "    - 모델 복잡도는 model1 < model2 < model3 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0769324113849796 0.05711125441447511 0.08956654762313918\n"
     ]
    }
   ],
   "source": [
    "print(mae1, mae2, mae3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 은닉층의 구조가 (10,10)인 모델의 성능이 가장 좋음\n",
    "- 은닉층의 구조는 모델의 복잡도와 직결되므로 더 좋은 은닉층의 구조를 찾으려면 (5,)보다는 복잡하고 (100,100,100)보다는 단순한 구조를 탐색해야 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer_sizes 인자를 바꿔가며 다양한 모델 구조를 정의하고 그 성능을 확인해보겠습니다.\n",
    "\n",
    "모델 구조 비교 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = NN(hidden_layer_sizes=(15,15), random_state=2022).fit(X_train, y_train)\n",
    "model5 = NN(hidden_layer_sizes=(10,5), random_state=2022).fit(X_train, y_train)\n",
    "model6 = NN(hidden_layer_sizes=(20,), random_state=2022).fit(X_train, y_train)\n",
    "\n",
    "y4_pred = model4.predict(X_test)\n",
    "y5_pred = model5.predict(X_test)\n",
    "y6_pred = model6.predict(X_test)\n",
    "\n",
    "mae4 = MAE(y_test, y4_pred)\n",
    "mae5 = MAE(y_test, y5_pred)\n",
    "mae6 = MAE(y_test, y6_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05044311790884915 0.04886901801873948 0.05072093415834021\n"
     ]
    }
   ],
   "source": [
    "print(mae4, mae5, mae6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (10,5)에서 가장 좋은 성능을 보였으며, model4, model5, model6 모두 model2 보다 나은 성능을 보임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최대 이터레이션 횟수 설정\n",
    "샘플 수가 적고 상대적으로 특징이 많은 데이터에 대해 최대 이터레이션 횟수를 설정해보겠습니다.\n",
    "\n",
    "최대 이터레이션 횟수 설정 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748.3964922052432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkfka\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/regression/baseball.csv')\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2022)\n",
    "\n",
    "model = NN(random_state=2022).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = MAE(y_test, y_pred)\n",
    "\n",
    "print(mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치가 수렴하지 못하고 최대 이터레이션 횟수인 200에 도달해서 ConvergenceWarning이 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687.3320969527582\n"
     ]
    }
   ],
   "source": [
    "model = NN(random_state=2022, max_iter=10000).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = MAE(y_test, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ConvergenceWarning이 발생하지 않음\n",
    "- 오차가 크게 줄었음\n",
    "- 그러나 상황에 따라서는 수렴함으로써 과적합될 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "107109ef1f3d2c1d8ef2422d7f3c7553b0305ed5b701a76fbc5bdcb10e21ee2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
